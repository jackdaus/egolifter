{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7bb8714",
   "metadata": {},
   "source": [
    "# Example of gsa with sam2 (WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be12001a",
   "metadata": {},
   "source": [
    "## Reproduce sam1 processing\n",
    "\n",
    "A snippet of the the logic from `generate_gsa_results.py` script. This notebook are the minimal steps for making the basic sam(1.0) masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e0db6720",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADT_DIR             = '/home/ubuntu/cs-747-project/adt'\n",
    "ADT_PROCESSED_DIR   = '/home/ubuntu/cs-747-project/adt_processed_new'\n",
    "SCENE_NAME          = 'Apartment_release_golden_skeleton_seq100_10s_sample'\n",
    "SAM_ENCODER_VERSION = 'vit_h'\n",
    "SAM_CHECKPOINT_PATH = 'sam_vit_h_4b8939.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0db4da05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    }
   ],
   "source": [
    "from segment_anything import sam_model_registry\n",
    "from segment_anything import SamAutomaticMaskGenerator\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "# Get sam mask generator\n",
    "sam = sam_model_registry[SAM_ENCODER_VERSION](checkpoint=SAM_CHECKPOINT_PATH)\n",
    "sam.to(device)\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d363dd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "# define classes (from generate_gsa_results.py)\n",
    "class Dataset():\n",
    "    def __init__(self, args) -> None:\n",
    "        self.input_folder = args.input_folder\n",
    "        assert self.input_folder.exists(), f\"Input folder {self.input_folder} does not exist. \"\n",
    "\n",
    "        self.detection_save_folder = self.input_folder / f\"gsa_det_{args.class_set}_{args.sam_variant}\"\n",
    "        self.detection_save_folder.mkdir(exist_ok=True)\n",
    "\n",
    "        self.vis_save_folder = self.input_folder / f\"gsa_vis_{args.class_set}_{args.sam_variant}\"\n",
    "        self.vis_save_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Any:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class AriaDataset(Dataset):\n",
    "    def __init__(self, args: argparse.Namespace) -> None:\n",
    "        super().__init__(args)\n",
    "\n",
    "        transform_path = self.input_folder / \"transforms.json\"\n",
    "        with open(transform_path) as json_file:\n",
    "            frames = json.loads(json_file.read())[\"frames\"]\n",
    "        \n",
    "        # Only keep the RGB images\n",
    "        self.frames = [f for f in frames if f['camera_name'] == 'rgb']\n",
    "\n",
    "        self.frames.sort(key=lambda f: f[\"image_path\"])\n",
    "\n",
    "    def __getitem__(self, index: int) -> Any:\n",
    "        subpath = self.frames[index][\"image_path\"]\n",
    "        image_path = self.input_folder / subpath\n",
    "        image_filename = subpath[:-4] # remove the .png/.jpg extension\n",
    "\n",
    "        return image_path, image_filename\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b8a948b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# The SAM based on automatic mask generation, without bbox prompting\n",
    "def get_sam_segmentation_dense(\n",
    "    variant:str, model: Any, image: np.ndarray\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    '''\n",
    "    The SAM based on automatic mask generation, without bbox prompting\n",
    "    \n",
    "    Args:\n",
    "        model: The mask generator or the YOLO model\n",
    "        image: )H, W, 3), in RGB color space, in range [0, 255]\n",
    "        \n",
    "    Returns:\n",
    "        mask: (N, H, W)\n",
    "        xyxy: (N, 4)\n",
    "        conf: (N,)\n",
    "    '''\n",
    "    if variant == \"sam\":\n",
    "        results = model.generate(image)\n",
    "        mask = []\n",
    "        xyxy = []\n",
    "        conf = []\n",
    "        for r in results:\n",
    "            mask.append(r[\"segmentation\"])\n",
    "            r_xyxy = r[\"bbox\"].copy()\n",
    "            # Convert from xyhw format to xyxy format\n",
    "            r_xyxy[2] += r_xyxy[0]\n",
    "            r_xyxy[3] += r_xyxy[1]\n",
    "            xyxy.append(r_xyxy)\n",
    "            conf.append(r[\"predicted_iou\"])\n",
    "        mask = np.array(mask)\n",
    "        xyxy = np.array(xyxy)\n",
    "        conf = np.array(conf)\n",
    "        return mask, xyxy, conf\n",
    "    elif variant == \"fastsam\":\n",
    "        # The arguments are directly copied from the GSA repo\n",
    "        results = model(\n",
    "            image,\n",
    "            imgsz=1024,\n",
    "            device=\"cuda\",\n",
    "            retina_masks=True,\n",
    "            iou=0.9,\n",
    "            conf=0.4,\n",
    "            max_det=100,\n",
    "        )\n",
    "        raise NotImplementedError\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e94ebb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "from supervision.draw.color import Color, ColorPalette\n",
    "import dataclasses\n",
    "\n",
    "def vis_result_fast(\n",
    "    image: np.ndarray, \n",
    "    detections: sv.Detections, \n",
    "    classes: list[str], \n",
    "    color: Color | ColorPalette = ColorPalette.DEFAULT, \n",
    "    instance_random_color: bool = False,\n",
    "    draw_bbox: bool = True,\n",
    ") -> np.ndarray:\n",
    "    '''\n",
    "    Annotate the image with the detection results. \n",
    "    This is fast but of the same resolution of the input image, thus can be blurry. \n",
    "    '''\n",
    "    # annotate image with detections\n",
    "    box_annotator = sv.BoxAnnotator(\n",
    "        color = color,\n",
    "    )\n",
    "    label_annontator = sv.LabelAnnotator(\n",
    "        text_scale=0.3,\n",
    "        text_thickness=1,\n",
    "        text_padding=2,\n",
    "    )\n",
    "    mask_annotator = sv.MaskAnnotator(\n",
    "        color = color,\n",
    "        opacity=0.35,\n",
    "    )\n",
    "    labels = [\n",
    "        f\"{classes[class_id]} {confidence:0.2f}\" \n",
    "        for _, _, confidence, class_id, _, _\n",
    "        in detections]\n",
    "    \n",
    "    if instance_random_color:\n",
    "        # generate random colors for each segmentation\n",
    "        # First create a shallow copy of the input detections\n",
    "        detections = dataclasses.replace(detections)\n",
    "        detections.class_id = np.arange(len(detections))\n",
    "        \n",
    "    annotated_image = mask_annotator.annotate(scene=image.copy(), detections=detections)\n",
    "    \n",
    "    if draw_bbox:\n",
    "        annotated_image = box_annotator.annotate(scene=annotated_image, detections=detections)\n",
    "        annotated_image = label_annontator.annotate(scene=annotated_image, detections=detections, labels=labels)\n",
    "    return annotated_image, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3519ed3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input folder: /home/ubuntu/cs-747-project/adt_processed_new/Apartment_release_golden_skeleton_seq100_10s_sample\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "input_folder = (Path(ADT_PROCESSED_DIR) / SCENE_NAME)\n",
    "print(f\"Input folder: {input_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "527086b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found global_points.csv.gz file, assuming Aria data set!\n"
     ]
    }
   ],
   "source": [
    "classes = ['item']\n",
    "##### initialize the dataset #####\n",
    "rotate_back = False\n",
    "if (input_folder / \"global_points.csv.gz\").exists():\n",
    "    print(f\"Found global_points.csv.gz file, assuming Aria data set!\")\n",
    "    # dataset = AriaDataset(args)\n",
    "    dataset = AriaDataset(args=argparse.Namespace(\n",
    "        input_folder=input_folder,\n",
    "        class_set='none',\n",
    "        sam_variant='sam',\n",
    "    ))\n",
    "    rotate_back = True\n",
    "else:\n",
    "    # Not implemented yet\n",
    "    raise NotImplementedError(\"Only Aria data set is supported for now!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ac33376f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 267/267 [11:36<00:00,  2.61s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "import numpy as np\n",
    "import imageio\n",
    "import supervision as sv\n",
    "import pickle\n",
    "import gzip\n",
    "from PIL import Image\n",
    "\n",
    "stride =1\n",
    "# TODO experiment with longer side... might be causing aliasing issues?\n",
    "max_longer_side = 512\n",
    "\n",
    "annotated_frames = []\n",
    "global_classes = []\n",
    "\n",
    "for idx in trange(0, len(dataset), stride):\n",
    "    # image_path = args.input_folder / frames[idx][\"image_path\"]\n",
    "    # image_filename = image_path.name.split('.')[0]\n",
    "    image_path, image_filename = dataset[idx]\n",
    "\n",
    "    image_pil = Image.open(image_path)\n",
    "    # image_pil = image_pil.resize((args.output_width, args.output_height))\n",
    "    longer_side = min(max(image_pil.size), max_longer_side)\n",
    "    resize_scale = float(longer_side) / max(image_pil.size)\n",
    "    image_pil = image_pil.resize(\n",
    "        (int(image_pil.size[0] * resize_scale), int(image_pil.size[1] * resize_scale))\n",
    "    )\n",
    "    # If image is RGBA, drop the alpha channel\n",
    "    if image_pil.mode == \"RGBA\":\n",
    "        image_pil = image_pil.convert(\"RGB\")\n",
    "    \n",
    "    if rotate_back:\n",
    "        image_pil = image_pil.rotate(-90, expand=True)\n",
    "    image_rgb = np.array(image_pil)\n",
    "    image_bgr = image_rgb[:, :, ::-1].copy()\n",
    "\n",
    "    # add classes to global classes\n",
    "    for c in classes:\n",
    "        if c not in global_classes:\n",
    "            global_classes.append(c)\n",
    "    \n",
    "    # if args.accumu_classes:\n",
    "    #     # Use all the classes that have been seen so far\n",
    "    #     classes = global_classes\n",
    "\n",
    "    ### Detection and segmentation ###\n",
    "    # Directly use SAM in dense sampling mode to get segmentation\n",
    "    mask, xyxy, conf = get_sam_segmentation_dense(\n",
    "        'sam', mask_generator, image_rgb)\n",
    "\n",
    "    detections = sv.Detections(\n",
    "        xyxy=xyxy,\n",
    "        confidence=conf,\n",
    "        class_id=np.zeros_like(conf).astype(int),\n",
    "        mask=mask,\n",
    "    )\n",
    "\n",
    "    # Remove the bounding boxes that are too large (they tend to capture the entire image)\n",
    "    areas = (detections.xyxy[:, 2] - detections.xyxy[:, 0]) * (detections.xyxy[:, 3] - detections.xyxy[:, 1])\n",
    "    area_ratios = areas / (image_rgb.shape[0] * image_rgb.shape[1])\n",
    "    valid_idx = area_ratios < 0.6\n",
    "    detections.xyxy = detections.xyxy[valid_idx]\n",
    "    detections.confidence = detections.confidence[valid_idx]\n",
    "    detections.class_id = detections.class_id[valid_idx]\n",
    "    detections.mask = detections.mask[valid_idx]\n",
    "\n",
    "    ### Compute CLIP features ###\n",
    "    # if not args.no_clip:\n",
    "    #     image_crops, image_feats, text_feats = compute_clip_features(\n",
    "    #         image_rgb, detections, clip_model, clip_preprocess, clip_tokenizer, classes, args.device)\n",
    "    # else:\n",
    "    #     image_crops, image_feats, text_feats = None, None, None\n",
    "    image_crops, image_feats, text_feats = None, None, None\n",
    "\n",
    "    ### Save the detection results ###\n",
    "    detection_save_path = dataset.detection_save_folder / f\"{image_filename}.pkl.gz\"\n",
    "    detection_save_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "    det_results = {\n",
    "        \"image_path\": image_path,\n",
    "        \"xyxy\": detections.xyxy,\n",
    "        \"confidence\": detections.confidence,\n",
    "        \"class_id\": detections.class_id,\n",
    "        \"mask\": detections.mask,\n",
    "        \"classes\": classes,\n",
    "        \"image_crops\": image_crops,\n",
    "        \"image_feats\": image_feats,\n",
    "        \"text_feats\": text_feats,\n",
    "    }\n",
    "    with gzip.open(str(detection_save_path), 'wb') as f:\n",
    "        pickle.dump(det_results, f)\n",
    "\n",
    "        \n",
    "    ### Visualize results and save ###\n",
    "    annotated_image, labels = vis_result_fast(\n",
    "        image_rgb, detections, classes, \n",
    "        # instance_random_color = args.class_set==\"none\",\n",
    "        instance_random_color = True,\n",
    "        # draw_bbox = args.class_set!=\"none\",\n",
    "        draw_bbox = False,\n",
    "    )\n",
    "\n",
    "    vis_save_path = dataset.vis_save_folder / f\"{image_filename}.png\"\n",
    "    vis_save_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "    imageio.imwrite(vis_save_path, annotated_image)\n",
    "    \n",
    "    # plt.figure(figsize=(10, 10))\n",
    "    # plt.imshow(annotated_image)\n",
    "    # plt.title(f\"Frame {idx}\")\n",
    "    # plt.show()\n",
    "    # cv2.imwrite(vis_save_path, annotated_image)\n",
    "    annotated_frames.append(annotated_image)\n",
    "\n",
    "# Save the annotated frames as a video\n",
    "annotated_frames = np.stack(annotated_frames, axis=0)\n",
    "\n",
    "imageio.mimwrite(\n",
    "    input_folder / f\"gsa_vis_none_sam.mp4\",\n",
    "    annotated_frames,\n",
    "    fps=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1fff41",
   "metadata": {},
   "source": [
    "## sam2\n",
    "\n",
    "Now we adapt the above workflow to work with sam2. (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c9b39b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egolifter",
   "language": "python",
   "name": "egolifter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
