{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7bb8714",
   "metadata": {},
   "source": [
    "# Reproduce sam1 processing\n",
    "    \n",
    "A snippet of the the logic from `generate_gsa_results.py` script. This notebook are the minimal steps for making the basic sam(1.0) masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0db6720",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADT_DIR             = '/home/ubuntu/cs-747-project/adt'\n",
    "ADT_PROCESSED_DIR   = '/home/ubuntu/cs-747-project/adt_processed_new'\n",
    "SCENE_NAME          = 'Apartment_release_golden_skeleton_seq100_10s_sample'\n",
    "SAM_ENCODER_VERSION = 'vit_h'\n",
    "SAM_CHECKPOINT_PATH = 'sam_vit_h_4b8939.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0db4da05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/egolifter/.venv/lib/python3.10/site-packages/segment_anything/build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    }
   ],
   "source": [
    "from segment_anything import sam_model_registry\n",
    "from segment_anything import SamAutomaticMaskGenerator\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "# Get sam mask generator\n",
    "sam = sam_model_registry[SAM_ENCODER_VERSION](checkpoint=SAM_CHECKPOINT_PATH)\n",
    "sam.to(device)\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d363dd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "# define classes (from generate_gsa_results.py)\n",
    "class Dataset():\n",
    "    def __init__(self, args) -> None:\n",
    "        self.input_folder = args.input_folder\n",
    "        assert self.input_folder.exists(), f\"Input folder {self.input_folder} does not exist. \"\n",
    "\n",
    "        self.detection_save_folder = self.input_folder / f\"gsa_det_{args.class_set}_{args.sam_variant}\"\n",
    "        self.detection_save_folder.mkdir(exist_ok=True)\n",
    "\n",
    "        self.vis_save_folder = self.input_folder / f\"gsa_vis_{args.class_set}_{args.sam_variant}\"\n",
    "        self.vis_save_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Any:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class AriaDataset(Dataset):\n",
    "    def __init__(self, args: argparse.Namespace) -> None:\n",
    "        super().__init__(args)\n",
    "\n",
    "        transform_path = self.input_folder / \"transforms.json\"\n",
    "        with open(transform_path) as json_file:\n",
    "            frames = json.loads(json_file.read())[\"frames\"]\n",
    "        \n",
    "        # Only keep the RGB images\n",
    "        self.frames = [f for f in frames if f['camera_name'] == 'rgb']\n",
    "\n",
    "        self.frames.sort(key=lambda f: f[\"image_path\"])\n",
    "\n",
    "    def __getitem__(self, index: int) -> Any:\n",
    "        subpath = self.frames[index][\"image_path\"]\n",
    "        image_path = self.input_folder / subpath\n",
    "        image_filename = subpath[:-4] # remove the .png/.jpg extension\n",
    "\n",
    "        return image_path, image_filename\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8a948b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# The SAM based on automatic mask generation, without bbox prompting\n",
    "def get_sam_segmentation_dense(\n",
    "    variant:str, model: Any, image: np.ndarray\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    '''\n",
    "    The SAM based on automatic mask generation, without bbox prompting\n",
    "    \n",
    "    Args:\n",
    "        model: The mask generator or the YOLO model\n",
    "        image: )H, W, 3), in RGB color space, in range [0, 255]\n",
    "        \n",
    "    Returns:\n",
    "        mask: (N, H, W)\n",
    "        xyxy: (N, 4)\n",
    "        conf: (N,)\n",
    "    '''\n",
    "    if variant == \"sam\":\n",
    "        results = model.generate(image)\n",
    "        mask = []\n",
    "        xyxy = []\n",
    "        conf = []\n",
    "        for r in results:\n",
    "            mask.append(r[\"segmentation\"])\n",
    "            r_xyxy = r[\"bbox\"].copy()\n",
    "            # Convert from xyhw format to xyxy format\n",
    "            r_xyxy[2] += r_xyxy[0]\n",
    "            r_xyxy[3] += r_xyxy[1]\n",
    "            xyxy.append(r_xyxy)\n",
    "            conf.append(r[\"predicted_iou\"])\n",
    "        mask = np.array(mask)\n",
    "        xyxy = np.array(xyxy)\n",
    "        conf = np.array(conf)\n",
    "        return mask, xyxy, conf\n",
    "    elif variant == \"fastsam\":\n",
    "        # The arguments are directly copied from the GSA repo\n",
    "        results = model(\n",
    "            image,\n",
    "            imgsz=1024,\n",
    "            device=\"cuda\",\n",
    "            retina_masks=True,\n",
    "            iou=0.9,\n",
    "            conf=0.4,\n",
    "            max_det=100,\n",
    "        )\n",
    "        raise NotImplementedError\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e94ebb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "from supervision.draw.color import Color, ColorPalette\n",
    "import dataclasses\n",
    "\n",
    "def vis_result_fast(\n",
    "    image: np.ndarray, \n",
    "    detections: sv.Detections, \n",
    "    classes: list[str], \n",
    "    color: Color | ColorPalette = ColorPalette.DEFAULT, \n",
    "    instance_random_color: bool = False,\n",
    "    draw_bbox: bool = True,\n",
    ") -> np.ndarray:\n",
    "    '''\n",
    "    Annotate the image with the detection results. \n",
    "    This is fast but of the same resolution of the input image, thus can be blurry. \n",
    "    '''\n",
    "    # annotate image with detections\n",
    "    box_annotator = sv.BoxAnnotator(\n",
    "        color = color,\n",
    "    )\n",
    "    label_annontator = sv.LabelAnnotator(\n",
    "        text_scale=0.3,\n",
    "        text_thickness=1,\n",
    "        text_padding=2,\n",
    "    )\n",
    "    mask_annotator = sv.MaskAnnotator(\n",
    "        color = color,\n",
    "        opacity=0.35,\n",
    "    )\n",
    "    labels = [\n",
    "        f\"{classes[class_id]} {confidence:0.2f}\" \n",
    "        for _, _, confidence, class_id, _, _\n",
    "        in detections]\n",
    "    \n",
    "    if instance_random_color:\n",
    "        # generate random colors for each segmentation\n",
    "        # First create a shallow copy of the input detections\n",
    "        detections = dataclasses.replace(detections)\n",
    "        detections.class_id = np.arange(len(detections))\n",
    "        \n",
    "    annotated_image = mask_annotator.annotate(scene=image.copy(), detections=detections)\n",
    "    \n",
    "    if draw_bbox:\n",
    "        annotated_image = box_annotator.annotate(scene=annotated_image, detections=detections)\n",
    "        annotated_image = label_annontator.annotate(scene=annotated_image, detections=detections, labels=labels)\n",
    "    return annotated_image, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3519ed3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input folder: /home/ubuntu/cs-747-project/adt_processed_new/Apartment_release_golden_skeleton_seq100_10s_sample\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "input_folder = (Path(ADT_PROCESSED_DIR) / SCENE_NAME)\n",
    "print(f\"Input folder: {input_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "527086b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found global_points.csv.gz file, assuming Aria data set!\n"
     ]
    }
   ],
   "source": [
    "classes = ['item']\n",
    "##### initialize the dataset #####\n",
    "rotate_back = False\n",
    "if (input_folder / \"global_points.csv.gz\").exists():\n",
    "    print(f\"Found global_points.csv.gz file, assuming Aria data set!\")\n",
    "    # dataset = AriaDataset(args)\n",
    "    dataset = AriaDataset(args=argparse.Namespace(\n",
    "        input_folder=input_folder,\n",
    "        class_set='none',\n",
    "        sam_variant='sam',\n",
    "    ))\n",
    "    rotate_back = True\n",
    "else:\n",
    "    # Not implemented yet\n",
    "    raise NotImplementedError(\"Only Aria data set is supported for now!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac33376f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/267 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 9/267 [00:31<14:49,  3.45s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 48\u001b[0m\n\u001b[1;32m     40\u001b[0m         global_classes\u001b[38;5;241m.\u001b[39mappend(c)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# if args.accumu_classes:\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#     # Use all the classes that have been seen so far\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#     classes = global_classes\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m### Detection and segmentation ###\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Directly use SAM in dense sampling mode to get segmentation\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m mask, xyxy, conf \u001b[38;5;241m=\u001b[39m \u001b[43mget_sam_segmentation_dense\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_rgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m detections \u001b[38;5;241m=\u001b[39m sv\u001b[38;5;241m.\u001b[39mDetections(\n\u001b[1;32m     52\u001b[0m     xyxy\u001b[38;5;241m=\u001b[39mxyxy,\n\u001b[1;32m     53\u001b[0m     confidence\u001b[38;5;241m=\u001b[39mconf,\n\u001b[1;32m     54\u001b[0m     class_id\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros_like(conf)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m),\n\u001b[1;32m     55\u001b[0m     mask\u001b[38;5;241m=\u001b[39mmask,\n\u001b[1;32m     56\u001b[0m )\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Remove the bounding boxes that are too large (they tend to capture the entire image)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m, in \u001b[0;36mget_sam_segmentation_dense\u001b[0;34m(variant, model, image)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03mThe SAM based on automatic mask generation, without bbox prompting\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    conf: (N,)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m variant \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msam\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 20\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     mask \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     22\u001b[0m     xyxy \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/egolifter/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/egolifter/.venv/lib/python3.10/site-packages/segment_anything/automatic_mask_generator.py:163\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator.generate\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03mGenerates masks for the given image.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m         the mask, given in XYWH format.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Generate masks\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m mask_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# Filter small disconnected regions and holes in masks\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_mask_region_area \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/egolifter/.venv/lib/python3.10/site-packages/segment_anything/automatic_mask_generator.py:206\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator._generate_masks\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    204\u001b[0m data \u001b[38;5;241m=\u001b[39m MaskData()\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m crop_box, layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(crop_boxes, layer_idxs):\n\u001b[0;32m--> 206\u001b[0m     crop_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_crop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_box\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     data\u001b[38;5;241m.\u001b[39mcat(crop_data)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# Remove duplicate masks between crops\u001b[39;00m\n",
      "File \u001b[0;32m~/egolifter/.venv/lib/python3.10/site-packages/segment_anything/automatic_mask_generator.py:245\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator._process_crop\u001b[0;34m(self, image, crop_box, crop_layer_idx, orig_size)\u001b[0m\n\u001b[1;32m    243\u001b[0m data \u001b[38;5;241m=\u001b[39m MaskData()\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (points,) \u001b[38;5;129;01min\u001b[39;00m batch_iterator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoints_per_batch, points_for_image):\n\u001b[0;32m--> 245\u001b[0m     batch_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcropped_im_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_box\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m     data\u001b[38;5;241m.\u001b[39mcat(batch_data)\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m batch_data\n",
      "File \u001b[0;32m~/egolifter/.venv/lib/python3.10/site-packages/segment_anything/automatic_mask_generator.py:297\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator._process_batch\u001b[0;34m(self, points, im_size, crop_box, orig_size)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_iou_thresh \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    296\u001b[0m     keep_mask \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miou_preds\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_iou_thresh\n\u001b[0;32m--> 297\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeep_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# Calculate stability score\u001b[39;00m\n\u001b[1;32m    300\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstability_score\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m calculate_stability_score(\n\u001b[1;32m    301\u001b[0m     data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmask_threshold, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstability_score_offset\n\u001b[1;32m    302\u001b[0m )\n",
      "File \u001b[0;32m~/egolifter/.venv/lib/python3.10/site-packages/segment_anything/utils/amg.py:49\u001b[0m, in \u001b[0;36mMaskData.filter\u001b[0;34m(self, keep)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stats[k] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stats[k] \u001b[38;5;241m=\u001b[39m v[torch\u001b[38;5;241m.\u001b[39mas_tensor(keep, device\u001b[38;5;241m=\u001b[39mv\u001b[38;5;241m.\u001b[39mdevice)]\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stats[k] \u001b[38;5;241m=\u001b[39m v[keep\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "import numpy as np\n",
    "import imageio\n",
    "import supervision as sv\n",
    "import pickle\n",
    "import gzip\n",
    "from PIL import Image\n",
    "\n",
    "stride =1\n",
    "# TODO experiment with longer side... might be causing aliasing issues?\n",
    "max_longer_side = 512\n",
    "\n",
    "annotated_frames = []\n",
    "global_classes = []\n",
    "\n",
    "for idx in trange(0, len(dataset), stride):\n",
    "    # image_path = args.input_folder / frames[idx][\"image_path\"]\n",
    "    # image_filename = image_path.name.split('.')[0]\n",
    "    image_path, image_filename = dataset[idx]\n",
    "\n",
    "    image_pil = Image.open(image_path)\n",
    "    # image_pil = image_pil.resize((args.output_width, args.output_height))\n",
    "    longer_side = min(max(image_pil.size), max_longer_side)\n",
    "    resize_scale = float(longer_side) / max(image_pil.size)\n",
    "    image_pil = image_pil.resize(\n",
    "        (int(image_pil.size[0] * resize_scale), int(image_pil.size[1] * resize_scale))\n",
    "    )\n",
    "    # If image is RGBA, drop the alpha channel\n",
    "    if image_pil.mode == \"RGBA\":\n",
    "        image_pil = image_pil.convert(\"RGB\")\n",
    "    \n",
    "    if rotate_back:\n",
    "        image_pil = image_pil.rotate(-90, expand=True)\n",
    "    image_rgb = np.array(image_pil)\n",
    "    image_bgr = image_rgb[:, :, ::-1].copy()\n",
    "\n",
    "    # add classes to global classes\n",
    "    for c in classes:\n",
    "        if c not in global_classes:\n",
    "            global_classes.append(c)\n",
    "    \n",
    "    # if args.accumu_classes:\n",
    "    #     # Use all the classes that have been seen so far\n",
    "    #     classes = global_classes\n",
    "\n",
    "    ### Detection and segmentation ###\n",
    "    # Directly use SAM in dense sampling mode to get segmentation\n",
    "    mask, xyxy, conf = get_sam_segmentation_dense(\n",
    "        'sam', mask_generator, image_rgb)\n",
    "\n",
    "    detections = sv.Detections(\n",
    "        xyxy=xyxy,\n",
    "        confidence=conf,\n",
    "        class_id=np.zeros_like(conf).astype(int),\n",
    "        mask=mask,\n",
    "    )\n",
    "\n",
    "    # Remove the bounding boxes that are too large (they tend to capture the entire image)\n",
    "    areas = (detections.xyxy[:, 2] - detections.xyxy[:, 0]) * (detections.xyxy[:, 3] - detections.xyxy[:, 1])\n",
    "    area_ratios = areas / (image_rgb.shape[0] * image_rgb.shape[1])\n",
    "    valid_idx = area_ratios < 0.6\n",
    "    detections.xyxy = detections.xyxy[valid_idx]\n",
    "    detections.confidence = detections.confidence[valid_idx]\n",
    "    detections.class_id = detections.class_id[valid_idx]\n",
    "    detections.mask = detections.mask[valid_idx]\n",
    "\n",
    "    ### Compute CLIP features ###\n",
    "    # if not args.no_clip:\n",
    "    #     image_crops, image_feats, text_feats = compute_clip_features(\n",
    "    #         image_rgb, detections, clip_model, clip_preprocess, clip_tokenizer, classes, args.device)\n",
    "    # else:\n",
    "    #     image_crops, image_feats, text_feats = None, None, None\n",
    "    image_crops, image_feats, text_feats = None, None, None\n",
    "\n",
    "    ### Save the detection results ###\n",
    "    detection_save_path = dataset.detection_save_folder / f\"{image_filename}.pkl.gz\"\n",
    "    detection_save_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "    det_results = {\n",
    "        \"image_path\": image_path,\n",
    "        \"xyxy\": detections.xyxy,\n",
    "        \"confidence\": detections.confidence,\n",
    "        \"class_id\": detections.class_id,\n",
    "        \"mask\": detections.mask,\n",
    "        \"classes\": classes,\n",
    "        \"image_crops\": image_crops,\n",
    "        \"image_feats\": image_feats,\n",
    "        \"text_feats\": text_feats,\n",
    "    }\n",
    "    with gzip.open(str(detection_save_path), 'wb') as f:\n",
    "        pickle.dump(det_results, f)\n",
    "\n",
    "        \n",
    "    ### Visualize results and save ###\n",
    "    annotated_image, labels = vis_result_fast(\n",
    "        image_rgb, detections, classes, \n",
    "        # instance_random_color = args.class_set==\"none\",\n",
    "        instance_random_color = True,\n",
    "        # draw_bbox = args.class_set!=\"none\",\n",
    "        draw_bbox = False,\n",
    "    )\n",
    "\n",
    "    vis_save_path = dataset.vis_save_folder / f\"{image_filename}.png\"\n",
    "    vis_save_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "    imageio.imwrite(vis_save_path, annotated_image)\n",
    "    \n",
    "    # plt.figure(figsize=(10, 10))\n",
    "    # plt.imshow(annotated_image)\n",
    "    # plt.title(f\"Frame {idx}\")\n",
    "    # plt.show()\n",
    "    # cv2.imwrite(vis_save_path, annotated_image)\n",
    "    annotated_frames.append(annotated_image)\n",
    "\n",
    "# Save the annotated frames as a video\n",
    "annotated_frames = np.stack(annotated_frames, axis=0)\n",
    "\n",
    "imageio.mimwrite(\n",
    "    input_folder / f\"gsa_vis_none_sam.mp4\",\n",
    "    annotated_frames,\n",
    "    fps=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1fff41",
   "metadata": {},
   "source": [
    "## sam2\n",
    "\n",
    "Now we adapt the above workflow to work with sam2. (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c9b39b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egolifter",
   "language": "python",
   "name": "egolifter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
